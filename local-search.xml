<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2.CUDA并行程序设计</title>
    <link href="/2024/09/29/2.CUDA%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    <url>/2024/09/29/2.CUDA%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<blockquote><p> &lt;大规模并行处理器编程实战&gt;读书笔记</p></blockquote><h4 id="Intro-to-GPU"><a href="#Intro-to-GPU" class="headerlink" title="Intro to GPU"></a>Intro to GPU</h4><p>GPU是Graphics Processing Unit, 由于图形渲染任务具有高度的并行性, 这种具有高度并行化的设备就成了游戏等图形渲染任务的宠儿.</p><p>首先辨识一下GPU和CPU各自的擅长部分, 在实际场景选择硬件时需要考虑各自的优缺点:</p><table><thead><tr><th>GPU</th><th>CPU</th></tr></thead><tbody><tr><td>并行, 数据的吞吐量大</td><td>缩短单个串行程序的运行时间</td></tr><tr><td>适用于数据密集的计算</td><td>适用于低延迟的计算</td></tr></tbody></table><blockquote><p>为什么深度学习需要GPU: 举个简单的例子, 卷积计算是高度并行的, 也就是说数据之间没有先后的依赖性, 而且数据吞吐量需求大.往常的CPU尽管线程可以达到32(我的), 但是还是不如GPU的并行性, 下一节会提到.</p></blockquote><h4 id="GPU体系结构"><a href="#GPU体系结构" class="headerlink" title="GPU体系结构"></a>GPU体系结构</h4><p><img src="/../img/1.png" alt="1"></p><p>上图是一个GPU的基本架构, 可以先划分为两个部分, 一部分是global memory也就是储存数据的部分 , 另一部分是计算部分.</p><p>在计算部分 又可以继续细分为一个多层的结构, 最大的构建块, 构建块由两个叫做Streaming Multiprocessor(简称SM)多核的流处理器组成, SM还包含多个Streaming Processor(简称SP),就是那些最小的灰色方块,  是不是可以直观感受到线程数量之多.</p><blockquote><p>那如何使用这个机器, 是不是需要非常复杂的代码来对每个线程进行分配, 还是说批量分配, 下一节会提到</p></blockquote><h4 id="并行程序架构"><a href="#并行程序架构" class="headerlink" title="并行程序架构"></a>并行程序架构</h4><p>要使用GPU, 就不得不使用Cuda C语言(其实也还有其他的,比如OpenCL, 但是英伟达的芯片应用范围广, 且学习的是思维, 具体的语言只是载体), Cuda C是按照下图来调配GPU和CPU的</p><p><img src="/../img/2.png" alt="2"></p><p>首先, 我们的程序会在NVCC编译器上编译, 我们指示这个编译器, 哪些程序是分配给CPU运行的, 就交给主机(也就是上图的HOST), 分配给GPU的就交给显卡设备(是Device), 两者配合就达成了完整的程序效果.</p><p>上节说到并行的计算步骤就交给GPU, 比如我给你下面的问题</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">#知道了<span class="hljs-type">int</span> a[<span class="hljs-number">100</span>],b[<span class="hljs-number">100</span>];求<span class="hljs-type">int</span> c[<span class="hljs-number">100</span>],其中c[i]=a[i]+b[i] i=<span class="hljs-number">0.</span><span class="hljs-number">.99</span><br></code></pre></td></tr></table></figure><p>一般的处理方法是简单的,写一个循环就好了, 但是为了并行加速就可以交给GPU来做, 怎么指示他呢,直接上代码</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> <span class="hljs-title function_">vecAdd</span><span class="hljs-params">(<span class="hljs-type">int</span>* a,<span class="hljs-type">int</span>* b,<span class="hljs-type">int</span>* c,<span class="hljs-type">int</span> n)</span>&#123;<br>    #分配并且将数据从host转移到device<br><span class="hljs-type">int</span> size = n*<span class="hljs-keyword">sizeof</span>(<span class="hljs-type">int</span>);<br>    <span class="hljs-type">int</span> *d_a,*d_b,*d_c;<br>    cudaMalloc((<span class="hljs-type">void</span>**) &amp;d_a,size);<br>    cudaMemcpy(d_a,a,size,cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span>**) &amp;d_b,size);<br>    cudaMemcpy(d_b,b,size,cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span>**) &amp;d_c,size);<br>    #执行计算操作<br>    vecAddKernel&lt;&lt;&lt;<span class="hljs-built_in">ceil</span>(n/<span class="hljs-number">256</span>), <span class="hljs-number">256</span>&gt;&gt;&gt;(d_a,d_b,d_c,n);<br>    #将数据转移回来,得到结果<br>    cudaMemcpy(c,d_c,size,cudaMemcpyDeviceToHost);<br>    #清空GPU中分配的内存<br>    cudaFree(d_a);cudaFree(d_b);cudaFree(d_c);<br>&#125;<br></code></pre></td></tr></table></figure><p>很简单的, 分配地址–&gt;转移数据–&gt;计算–&gt;转移回来–&gt;free地址, 思路有了,让我们深入一下细节</p><ol><li><p>在<code>vecAddKernel&lt;&lt;&lt;ceil(n/256.0), 256&gt;&gt;&gt;(d_a,d_b,d_c,n);</code>中电脑如何知道交给GPU来算</p><p>第一个&lt;&lt;&lt;&gt;&gt;&gt;的声明,这个声明跟电脑说我这个由<code>ceil(n/256.0)</code>个线程块运行,每个线程块是<code>256</code>个线程, 算是<strong>配置参数</strong></p><p>其中<code>ceil(n/256.0)</code>就是说假如我有1000个并行的数据, 那这个算出来就是4, 需要申请四个线程块, 其中三个线程块是满的,最后一个有<code>256x4-1000=24</code>个线程没有被使用</p></li><li><p><strong>指针问题</strong>: 为什么<code>cudaMalloc((void**) &amp;d_b,size);</code>是因为cudaMalloc 需要一个指向指针的指针 (<code>void**</code>) 作为参数，它需要修改指针变量的值. </p><p>调用 <code>cudaMalloc</code> 之前，<code>d_b</code> 指针变量的值是未定义的 (或者指向某个无效的地址)。 <code>cudaMalloc</code> 函数会将分配的设备内存的起始地址写入 <code>d_b</code> 指针所指向的内存位置,这样我们就知道d_b被分配到哪里了.</p><p>而使用<code>void* d_b</code>看起来效果一样, 其实忽略了cudaMalloc的作用, 他不能修改地址,也就意味着他不能提供地址了.</p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
